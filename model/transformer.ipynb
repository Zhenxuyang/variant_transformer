{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#coding=utf-8\n","\n","'''\n","Enhancer Transformer的实现\n","'''\n","\n","from numpy.core.fromnumeric import var\n","import torch\n","from torch.nn.modules.loss import BCELoss\n","from torch.utils.data.dataloader import DataLoader\n","from torch import nn\n","import torch.nn.functional as F\n","import math\n","from torch.utils.data import Dataset, Subset\n","import pandas as pd\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class VariantDataset(Dataset):\n","    def __init__(self, file_path) -> None:\n","        self.raw_data = pd.read_csv(file_path)\n","        self.encode_seq()\n","\n","    def __len__(self):\n","        return self.raw_data.shape[0]\n","\n","    def __getitem__(self, index):\n","        variant = self.raw_data.iloc[index]\n","        ref = torch.tensor(variant['encoded_ref'])\n","        alt = torch.tensor(variant['encoded_alt'])\n","        label = torch.tensor(variant['label'])\n","        return [ref, alt, label]\n","    \n","    def encode_seq(self):\n","        # 编码\n","        dict_ = {\n","            'A': 1,\n","            'T': 2,\n","            'C': 3,\n","            'G': 4,\n","            'N': 0\n","        }\n","        self.raw_data['encoded_ref'] = self.raw_data['seq_101_ref'].apply(lambda x: list(map(lambda i: dict_[i], list(x))))\n","        self.raw_data['encoded_alt'] = self.raw_data['seq_101_alt'].apply(lambda x: list(map(lambda i: dict_[i], list(x))))\n","                \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TransformerEmbedding(nn.Module):\n","    def __init__(self, embedding_num, embedding_dim, pad_idx, device) -> None:\n","        super().__init__()\n","        self.embedding = nn.Embedding(embedding_num, embedding_dim, pad_idx, device=device)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    \"\"\"\n","    compute sinusoid encoding.\n","    \"\"\"\n","    def __init__(self, d_model, max_len, device):\n","        \"\"\"\n","        constructor of sinusoid encoding class\n","\n","        :param d_model: dimension of model\n","        :param max_len: max sequence length\n","        :param device: hardware device setting\n","        \"\"\"\n","        super(PositionalEncoding, self).__init__()\n","\n","        # same size with input matrix (for adding with input matrix)\n","        self.encoding = torch.zeros(max_len, d_model, device=device)\n","        self.encoding.requires_grad = False  # we don't need to compute gradient\n","\n","        pos = torch.arange(0, max_len, device=device)\n","        pos = pos.float().unsqueeze(dim=1)\n","        # 1D => 2D unsqueeze to represent word's position\n","\n","        _2i = torch.arange(0, d_model, step=2, device=device).float()\n","        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])\n","        # \"step=2\" means 'i' multiplied with two (same with 2 * i)\n","        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n","        if d_model % 2 == 1:\n","            _2i = _2i[:-1]\n","        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n","        # compute positional encoding to consider positional information of words\n","\n","    def forward(self, x):\n","        # self.encoding\n","        # [max_len = 512, d_model = 512]\n","\n","        batch_size, seq_len, d_model= x.size()\n","        # [batch_size = 128, seq_len = 30]\n","\n","        return x + self.encoding[:seq_len, :]\n","        # [seq_len = 30, d_model = 512]\n","        # it will add with tok_emb : [128, 30, 512]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","\n","    def __init__(self, d_model, n_head):\n","        super(MultiHeadAttention, self).__init__()\n","        self.n_head = n_head\n","        self.attention = ScaleDotProductAttention()\n","        self.w_q = nn.Linear(d_model, d_model)\n","        self.w_k = nn.Linear(d_model, d_model)\n","        self.w_v = nn.Linear(d_model, d_model)\n","        self.w_concat = nn.Linear(d_model, d_model)\n","\n","    def forward(self, q, k, v, mask=None):\n","        # 1. dot product with weight matrices\n","        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n","\n","        # 2. split tensor by number of heads\n","        q, k, v = self.split(q), self.split(k), self.split(v)\n","\n","        # 3. do scale dot product to compute similarity\n","        out, attention = self.attention(q, k, v, mask=mask)\n","        \n","        # 4. concat and pass to linear layer\n","        out = self.concat(out)\n","        out = self.w_concat(out)\n","\n","        # 5. visualize attention map\n","        # TODO : we should implement visualization\n","\n","        return out\n","\n","    def split(self, tensor):\n","        \"\"\"\n","        split tensor by number of head\n","\n","        :param tensor: [batch_size, length, d_model]\n","        :return: [batch_size, head, length, d_tensor]\n","        \"\"\"\n","        batch_size, length, d_model = tensor.size()\n","\n","        d_tensor = d_model // self.n_head\n","        tensor = tensor.view(batch_size, self.n_head, length, d_tensor)\n","        # it is similar with group convolution (split by number of heads)\n","\n","        return tensor\n","\n","    def concat(self, tensor):\n","        \"\"\"\n","        inverse function of self.split(tensor : torch.Tensor)\n","\n","        :param tensor: [batch_size, head, length, d_tensor]\n","        :return: [batch_size, length, d_model]\n","        \"\"\"\n","        batch_size, head, length, d_tensor = tensor.size()\n","        d_model = head * d_tensor\n","\n","        tensor = tensor.view(batch_size, length, d_model)\n","        return tensor\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ScaleDotProductAttention(nn.Module):\n","    \"\"\"\n","    compute scale dot product attention\n","\n","    Query : given sentence that we focused on (decoder)\n","    Key : every sentence to check relationship with Qeury(encoder)\n","    Value : every sentence same with Key (encoder)\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(ScaleDotProductAttention, self).__init__()\n","        self.softmax = nn.Softmax(-1)\n","\n","    def forward(self, q, k, v, mask=None, e=1e-12):\n","        # input is 4 dimension tensor\n","        # [batch_size, head, length, d_tensor]\n","        batch_size, head, length, d_tensor = k.size()\n","\n","        # 1. dot product Query with Key^T to compute similarity\n","        k_t = k.view(batch_size, head, d_tensor, length)  # transpose\n","        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n","\n","        # 2. apply masking (opt)\n","        if mask is not None:\n","            score = score.masked_fill(mask == 0, -e)\n","\n","        # 3. pass them softmax to make [0, 1] range\n","        score = self.softmax(score)\n","\n","        # 4. multiply with Value\n","        v = score @ v\n","\n","        return v, score\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    def __init__(self, d_model, eps=1e-12):\n","        super(LayerNorm, self).__init__()\n","        self.gamma = nn.Parameter(torch.ones(d_model))\n","        self.beta = nn.Parameter(torch.zeros(d_model))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        # '-1' means last dimension. \n","\n","        out = (x - mean) / (std + self.eps)\n","        out = self.gamma * out + self.beta\n","        return out\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class PositionwiseFeedForward(nn.Module):\n","\n","    def __init__(self, d_model, hidden, drop_prob=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.linear1 = nn.Linear(d_model, hidden)\n","        self.linear2 = nn.Linear(hidden, d_model)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=drop_prob)\n","\n","    def forward(self, x):\n","        x = self.linear1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.linear2(x)\n","        return x\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","\n","    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n","        super(EncoderLayer, self).__init__()\n","        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n","        self.norm1 = LayerNorm(d_model=d_model)\n","        self.dropout1 = nn.Dropout(p=drop_prob)\n","\n","        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n","        self.norm2 = LayerNorm(d_model=d_model)\n","        self.dropout2 = nn.Dropout(p=drop_prob)\n","\n","    def forward(self, x, src_mask):\n","        # 1. compute self attention\n","        _x = x\n","        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n","        \n","        # 2. add and norm\n","        x = self.norm1(x + _x)\n","        x = self.dropout1(x)\n","        \n","        # 3. positionwise feed forward network\n","        _x = x\n","        x = self.ffn(x)\n","      \n","        # 4. add and norm\n","        x = self.norm2(x + _x)\n","        x = self.dropout2(x)\n","        return x\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Encoder(nn.Module):\n","\n","    def __init__(self, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n","        super().__init__()\n","\n","        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n","                                                  ffn_hidden=ffn_hidden,\n","                                                  n_head=n_head,\n","                                                  drop_prob=drop_prob)\n","                                     for _ in range(n_layers)])\n","\n","    def forward(self, x, src_mask):\n","        for layer in self.layers:\n","            x = layer(x, src_mask)\n","\n","        return x\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ClassificationLayer(nn.Module):\n","    def __init__(self, embedding_dim, seq_len, output_dim) -> None:\n","        super().__init__()\n","        self.linear1 = nn.Linear(embedding_dim, 1)\n","        self.linear2 = nn.Linear(seq_len, output_dim)\n","        # self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        batch_size, _, _ = x.size() \n","        x = F.leaky_relu(self.linear1(x))\n","        x = x.view(batch_size, -1)\n","        x = F.sigmoid(self.linear2(x))\n","        # x = self.sigmoid(x)\n","        return x\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class VariantPathogenicityClassifier(nn.Module):\n","    def __init__(self, \n","        pad_idx,\n","        pad_size,\n","        embedding_dim,\n","        att_head_num,\n","        hidden_dim,\n","        device,\n","        n_layers=3,\n","        drop_prob=0.1,\n","        output_dim=20) -> None:\n","        super().__init__()\n","        self.pad_idx = pad_idx\n","        self.device = device\n","        self.positionalEncoding = PositionalEncoding(\n","            d_model=embedding_dim,\n","            max_len=pad_size,\n","            device=device\n","        )\n","        self.encoder = Encoder(\n","            d_model=embedding_dim,\n","            ffn_hidden=hidden_dim,\n","            n_head=att_head_num,\n","            n_layers=n_layers,\n","            drop_prob=drop_prob,\n","            device=device\n","        )\n","        self.classifier = ClassificationLayer(embedding_dim, pad_size, output_dim)\n","    \n","    def make_pad_mask(self, q, k, src_padding_idx):\n","        len_q, len_k = q.size(1), k.size(1)\n","\n","        # batch_size x 1 x 1 x len_k\n","        k = k.ne(src_padding_idx).unsqueeze(1).unsqueeze(2)\n","        # batch_size x 1 x len_q x len_k\n","        k = k.repeat(1, 1, len_q, 1)\n","\n","        # batch_size x 1 x len_q x 1\n","        q = q.ne(src_padding_idx).unsqueeze(1).unsqueeze(3)\n","        # batch_size x 1 x len_q x len_k\n","        q = q.repeat(1, 1, 1, len_k)\n","\n","        mask = k & q\n","        return mask\n","    \n","    def embedding(self, x, y, device):\n","        batch_size, length = x.size()\n","        x_ = torch.zeros(batch_size, length, 5).to(device)\n","        x = x.view(batch_size, -1, 1)\n","        x_.scatter_(2, x, 1)\n","        y_ = torch.zeros(batch_size, length, 5).to(device)\n","        y = y.view(batch_size, -1, 1)\n","        y_.scatter_(2, y, 1)\n","        return torch.cat((x_, y_), 2)\n","          \n","    def forward(self, x, y):\n","        src_mask = self.make_pad_mask(x, x, self.pad_idx)\n","        x = self.embedding(x, y, self.device)\n","        x = self.positionalEncoding(x)\n","        x = self.encoder(x, src_mask)\n","        x = self.classifier(x)\n","        return x\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 参数\n","\n","\n","# 类别\n","label_dim = 2\n","output_dim = 1\n","\n","# 序列文件路径\n","# file_path = '/kaggle/input/cancerenhancers/cancerEnhancers_filtered.csv'\n","file_path = \"../data/variants_with_seq.csv\"\n","\n","# pad的长度\n","pad_size = 101\n","# pad的填充索引\n","pad_idx = 0\n","\n","# embedding词典大小\n","embedding_num = 5\n","# embedding维度\n","embedding_dim = 10\n","\n","# encoder层数\n","n_layers = 3\n","\n","# feedfoward网络中隐藏层大小\n","hidden_dim = 32\n","# # 注意力的头数\n","att_head_num = 1\n","\n","# 学习率\n","learning_rate = 0.01\n","\n","# batch大小 \n","batch_size = 200\n","\n","# epoch数\n","epoch = 10\n","\n","# 打印间隔\n","log_interval = 100\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainset = VariantDataset('../data/train.csv')\n","validset = VariantDataset('../data/valid.csv')\n","trainLoader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n","validLoader = DataLoader(validset, batch_size=batch_size, shuffle=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = VariantPathogenicityClassifier(\n","    embedding_dim=embedding_dim,\n","    pad_idx=pad_idx,\n","    pad_size=pad_size,\n","    device=device,\n","    output_dim=output_dim,\n","    att_head_num=att_head_num,\n","    hidden_dim=hidden_dim,\n","    n_layers=n_layers\n",")\n","\n","model=model.to(device)\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","loss_func = BCELoss()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorboardX import SummaryWriter\n","writer = SummaryWriter()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (x, y, target) in enumerate(train_loader):\n","        x, y, target = x.to(device), y.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(x, y)\n","        target = torch.unsqueeze(target, 1)\n","        # output=output.to(torch.float32)\n","        target=target.to(torch.float32)\n","        loss = loss_func(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        writer.add_scalar('Train', loss, epoch)\n","        if batch_idx % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(x), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def valid(model, device, validloader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    threshold = 0.25\n","    with torch.no_grad():\n","        for x, y, target in validloader:\n","            x, y, target = x.to(device), y.to(device), target.to(device)\n","            output = model(x, y)\n","            target = torch.unsqueeze(target, 1)\n","            target = target.to(torch.float32)\n","            test_loss += F.binary_cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = (output > threshold).to(torch.float)\n","            pred = pred.reshape(1, -1)\n","            target = target.reshape(1, -1)\n","            correct += pred.eq(target).sum().item()\n","            # correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(validLoader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(validLoader.dataset),\n","        100. * correct / (len(validLoader.dataset))))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(epoch):\n","    print(i)\n","    train(model, device, trainLoader, optimizer, i)\n","    valid(model, device, validLoader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# valid(model, device, validLoader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# att_heads = [1, 3]\n","# hidden_dims = [128, 256, 512, 1024]\n","# layers = [2, 4, 6, 8, 10]\n","\n","# for head_num in att_heads:\n","#     for hidden_dim in hidden_dims:\n","#         for layer in layers:\n","#             print(head_num, hidden_dim, layer)\n","#             model = EnhancerClassifier(\n","#                 embedding_num = embedding_num,\n","#                 embedding_dim=embedding_dim,\n","#                 pad_idx=pad_idx,\n","#                 pad_size=pad_size,\n","#                 device=device,\n","#                 output_dim=label_dim,\n","#                 att_head_num=head_num,\n","#                 hidden_dim=hidden_dim,\n","#                 n_layers =layer\n","#             )\n","#             # %%\n","#             for i in range(epoch):\n","#                 print(\"-----epoch {}-----\".format(i))\n","#                 train(model, device, trainLoader, optimizer, i)\n","#                 valid(model, device, validLoader)\n","        \n","\n","\n","\n","\n","\n","\n"]}],"metadata":{"interpreter":{"hash":"796fac6b8cc66e26d7d5167379612d61b69e4c93c285e3908b5ef16019a4c00d"},"kernelspec":{"display_name":"Python 3.7.9 64-bit ('variant_transformer': virtualenv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
